{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IlzyIwxFUlJB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Conv2D,MaxPooling2D,Flatten,Dense,Activation,Dropout\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tflearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q--Mv-9vV19c",
        "outputId": "5ac1ce5f-204e-4819-d454-53aae2a877bf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 24.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=12bcd2fdfc9ff1c270d70bde026b55dfb40a349573b60c34cda7eb4f9abb0af9\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/14/2e/1d8e28cc47a5a931a2fb82438c9e37ef9246cc6a3774520271\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Data\n",
        "import tflearn.datasets.oxflower17 as oxflower17\n",
        "x, y = oxflower17.load_data(one_hot=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LmYzZE4V13X",
        "outputId": "dbfdaf1b-747b-4c3e-dbab-130f83785a14"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Oxford 17 category Flower Dataset, Please wait...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100.0% 60276736 / 60270631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Succesfully downloaded 17flowers.tgz 60270631 bytes.\n",
            "File Extracted\n",
            "Starting to parse images...\n",
            "Parsing Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzJWvX4XV1zg",
        "outputId": "1233dcea-c75f-44d6-9f88-8271b9eff5d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[0.07058824, 0.02745098, 0.04313726],\n",
              "         [0.04313726, 0.        , 0.01568628],\n",
              "         [0.05098039, 0.00784314, 0.02352941],\n",
              "         ...,\n",
              "         [0.22745098, 0.1254902 , 0.12941177],\n",
              "         [0.19215687, 0.10588235, 0.10196079],\n",
              "         [0.14117648, 0.0627451 , 0.05882353]],\n",
              "\n",
              "        [[0.07058824, 0.02745098, 0.04313726],\n",
              "         [0.04313726, 0.        , 0.01568628],\n",
              "         [0.05098039, 0.00784314, 0.02352941],\n",
              "         ...,\n",
              "         [0.2       , 0.10980392, 0.10980392],\n",
              "         [0.16470589, 0.08627451, 0.08235294],\n",
              "         [0.11764706, 0.03921569, 0.03529412]],\n",
              "\n",
              "        [[0.0627451 , 0.01960784, 0.03529412],\n",
              "         [0.03921569, 0.        , 0.01176471],\n",
              "         [0.05098039, 0.00784314, 0.02352941],\n",
              "         ...,\n",
              "         [0.16862746, 0.09411765, 0.09019608],\n",
              "         [0.14117648, 0.07450981, 0.06666667],\n",
              "         [0.09411765, 0.03137255, 0.02352941]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.5882353 , 0.62352943, 0.41960785],\n",
              "         [0.6313726 , 0.6666667 , 0.4745098 ],\n",
              "         [0.5764706 , 0.6039216 , 0.43529412],\n",
              "         ...,\n",
              "         [0.6745098 , 0.6431373 , 0.30588236],\n",
              "         [0.7921569 , 0.7607843 , 0.42352942],\n",
              "         [0.6862745 , 0.654902  , 0.32156864]],\n",
              "\n",
              "        [[0.5294118 , 0.56078434, 0.3882353 ],\n",
              "         [0.48235294, 0.5058824 , 0.34901962],\n",
              "         [0.38431373, 0.40784314, 0.26666668],\n",
              "         ...,\n",
              "         [0.69803923, 0.6745098 , 0.30588236],\n",
              "         [0.76862746, 0.7411765 , 0.37254903],\n",
              "         [0.74509805, 0.72156864, 0.3529412 ]],\n",
              "\n",
              "        [[0.29411766, 0.31764707, 0.18431373],\n",
              "         [0.21176471, 0.23137255, 0.10980392],\n",
              "         [0.12156863, 0.13725491, 0.03137255],\n",
              "         ...,\n",
              "         [0.7019608 , 0.6784314 , 0.2901961 ],\n",
              "         [0.6862745 , 0.6666667 , 0.27450982],\n",
              "         [0.74509805, 0.72156864, 0.33333334]]],\n",
              "\n",
              "\n",
              "       [[[0.04705882, 0.5411765 , 0.15294118],\n",
              "         [0.05490196, 0.5411765 , 0.15686275],\n",
              "         [0.0627451 , 0.5411765 , 0.16078432],\n",
              "         ...,\n",
              "         [0.5882353 , 0.8352941 , 0.5176471 ],\n",
              "         [0.59607846, 0.8156863 , 0.5137255 ],\n",
              "         [0.62352943, 0.83137256, 0.5372549 ]],\n",
              "\n",
              "        [[0.0627451 , 0.5568628 , 0.16862746],\n",
              "         [0.0627451 , 0.5529412 , 0.16862746],\n",
              "         [0.07058824, 0.54901963, 0.16470589],\n",
              "         ...,\n",
              "         [0.5647059 , 0.8156863 , 0.49803922],\n",
              "         [0.56078434, 0.78431374, 0.48235294],\n",
              "         [0.58431375, 0.79607844, 0.5058824 ]],\n",
              "\n",
              "        [[0.07058824, 0.5686275 , 0.18039216],\n",
              "         [0.07058824, 0.56078434, 0.17254902],\n",
              "         [0.07058824, 0.54509807, 0.16470589],\n",
              "         ...,\n",
              "         [0.5372549 , 0.8039216 , 0.47843137],\n",
              "         [0.54509807, 0.7764706 , 0.4745098 ],\n",
              "         [0.5764706 , 0.79607844, 0.5019608 ]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.04705882, 0.2       , 0.01176471],\n",
              "         [0.05490196, 0.20392157, 0.01960784],\n",
              "         [0.07058824, 0.21960784, 0.03529412],\n",
              "         ...,\n",
              "         [0.05490196, 0.1254902 , 0.04313726],\n",
              "         [0.05098039, 0.12156863, 0.03137255],\n",
              "         [0.2       , 0.27058825, 0.1764706 ]],\n",
              "\n",
              "        [[0.03529412, 0.18039216, 0.        ],\n",
              "         [0.03921569, 0.1882353 , 0.00392157],\n",
              "         [0.05882353, 0.20784314, 0.02352941],\n",
              "         ...,\n",
              "         [0.04705882, 0.16078432, 0.05882353],\n",
              "         [0.04313726, 0.14509805, 0.03921569],\n",
              "         [0.1882353 , 0.28627452, 0.18039216]],\n",
              "\n",
              "        [[0.19607843, 0.34509805, 0.16078432],\n",
              "         [0.20784314, 0.35686275, 0.17254902],\n",
              "         [0.21960784, 0.36862746, 0.18431373],\n",
              "         ...,\n",
              "         [0.16078432, 0.30980393, 0.2       ],\n",
              "         [0.16078432, 0.29411766, 0.18039216],\n",
              "         [0.30980393, 0.4392157 , 0.3254902 ]]],\n",
              "\n",
              "\n",
              "       [[[0.3882353 , 0.3647059 , 0.39215687],\n",
              "         [0.35686275, 0.35686275, 0.38431373],\n",
              "         [0.3254902 , 0.3647059 , 0.38431373],\n",
              "         ...,\n",
              "         [0.29803923, 0.48235294, 0.3137255 ],\n",
              "         [0.31764707, 0.4862745 , 0.34509805],\n",
              "         [0.33333334, 0.49411765, 0.35686275]],\n",
              "\n",
              "        [[0.3882353 , 0.37254903, 0.4117647 ],\n",
              "         [0.3647059 , 0.36862746, 0.40392157],\n",
              "         [0.34117648, 0.38039216, 0.40392157],\n",
              "         ...,\n",
              "         [0.3254902 , 0.5058824 , 0.34117648],\n",
              "         [0.34509805, 0.5058824 , 0.36862746],\n",
              "         [0.3529412 , 0.50980395, 0.38039216]],\n",
              "\n",
              "        [[0.38431373, 0.38039216, 0.42745098],\n",
              "         [0.37254903, 0.38431373, 0.42745098],\n",
              "         [0.35686275, 0.40392157, 0.43137255],\n",
              "         ...,\n",
              "         [0.34901962, 0.52156866, 0.37254903],\n",
              "         [0.3764706 , 0.5254902 , 0.4       ],\n",
              "         [0.3882353 , 0.53333336, 0.41568628]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.01960784, 0.16470589, 0.09411765],\n",
              "         [0.02352941, 0.16862746, 0.09803922],\n",
              "         [0.03529412, 0.1764706 , 0.10588235],\n",
              "         ...,\n",
              "         [0.0627451 , 0.23529412, 0.14901961],\n",
              "         [0.07843138, 0.24705882, 0.14901961],\n",
              "         [0.08627451, 0.2509804 , 0.15294118]],\n",
              "\n",
              "        [[0.00784314, 0.16470589, 0.08627451],\n",
              "         [0.01176471, 0.16862746, 0.09411765],\n",
              "         [0.03137255, 0.18039216, 0.10588235],\n",
              "         ...,\n",
              "         [0.03921569, 0.19215687, 0.10588235],\n",
              "         [0.05098039, 0.20784314, 0.11764706],\n",
              "         [0.05490196, 0.21176471, 0.12156863]],\n",
              "\n",
              "        [[0.        , 0.16078432, 0.08235294],\n",
              "         [0.00392157, 0.16862746, 0.08627451],\n",
              "         [0.02745098, 0.1882353 , 0.11372549],\n",
              "         ...,\n",
              "         [0.01960784, 0.15294118, 0.07450981],\n",
              "         [0.01568628, 0.15686275, 0.08235294],\n",
              "         [0.01568628, 0.16470589, 0.08627451]]],\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "       [[[0.6901961 , 0.65882355, 0.46666667],\n",
              "         [0.68235296, 0.6509804 , 0.46666667],\n",
              "         [0.6745098 , 0.6431373 , 0.46666667],\n",
              "         ...,\n",
              "         [0.62352943, 0.63529414, 0.46666667],\n",
              "         [0.6       , 0.6117647 , 0.44313726],\n",
              "         [0.5803922 , 0.6039216 , 0.42352942]],\n",
              "\n",
              "        [[0.68235296, 0.6509804 , 0.45882353],\n",
              "         [0.6745098 , 0.6431373 , 0.45882353],\n",
              "         [0.6666667 , 0.63529414, 0.45882353],\n",
              "         ...,\n",
              "         [0.6313726 , 0.63529414, 0.4745098 ],\n",
              "         [0.6039216 , 0.6117647 , 0.44313726],\n",
              "         [0.5882353 , 0.6       , 0.42352942]],\n",
              "\n",
              "        [[0.67058825, 0.6392157 , 0.44705883],\n",
              "         [0.6666667 , 0.63529414, 0.4509804 ],\n",
              "         [0.65882355, 0.62352943, 0.44705883],\n",
              "         ...,\n",
              "         [0.6392157 , 0.6431373 , 0.47843137],\n",
              "         [0.60784316, 0.6117647 , 0.44705883],\n",
              "         [0.5882353 , 0.6       , 0.42352942]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.23529412, 0.20392157, 0.16078432],\n",
              "         [0.18039216, 0.15686275, 0.10588235],\n",
              "         [0.11372549, 0.10588235, 0.04313726],\n",
              "         ...,\n",
              "         [0.47058824, 0.5137255 , 0.26666668],\n",
              "         [0.52156866, 0.56078434, 0.30980393],\n",
              "         [0.5411765 , 0.5764706 , 0.31764707]],\n",
              "\n",
              "        [[0.24313726, 0.21960784, 0.18039216],\n",
              "         [0.20392157, 0.19215687, 0.14117648],\n",
              "         [0.13725491, 0.13725491, 0.07450981],\n",
              "         ...,\n",
              "         [0.4745098 , 0.5294118 , 0.25490198],\n",
              "         [0.5294118 , 0.5803922 , 0.30980393],\n",
              "         [0.54901963, 0.6       , 0.32941177]],\n",
              "\n",
              "        [[0.2509804 , 0.23529412, 0.19215687],\n",
              "         [0.22745098, 0.21960784, 0.16862746],\n",
              "         [0.18039216, 0.18039216, 0.12156863],\n",
              "         ...,\n",
              "         [0.48235294, 0.5411765 , 0.2627451 ],\n",
              "         [0.5411765 , 0.5921569 , 0.31764707],\n",
              "         [0.56078434, 0.6117647 , 0.34117648]]],\n",
              "\n",
              "\n",
              "       [[[0.19607843, 0.15686275, 0.12156863],\n",
              "         [0.19607843, 0.15686275, 0.12156863],\n",
              "         [0.19215687, 0.15686275, 0.12156863],\n",
              "         ...,\n",
              "         [0.11372549, 0.0627451 , 0.09411765],\n",
              "         [0.14509805, 0.07843138, 0.09411765],\n",
              "         [0.1764706 , 0.10588235, 0.11372549]],\n",
              "\n",
              "        [[0.23529412, 0.1882353 , 0.13725491],\n",
              "         [0.24313726, 0.19215687, 0.14117648],\n",
              "         [0.24705882, 0.19607843, 0.14901961],\n",
              "         ...,\n",
              "         [0.10196079, 0.05490196, 0.07843138],\n",
              "         [0.13333334, 0.07058824, 0.08627451],\n",
              "         [0.16078432, 0.09803922, 0.11372549]],\n",
              "\n",
              "        [[0.3019608 , 0.23137255, 0.15294118],\n",
              "         [0.30588236, 0.23529412, 0.15686275],\n",
              "         [0.30588236, 0.23529412, 0.15686275],\n",
              "         ...,\n",
              "         [0.09411765, 0.05490196, 0.0627451 ],\n",
              "         [0.12156863, 0.07450981, 0.09019608],\n",
              "         [0.14117648, 0.09411765, 0.11372549]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.03137255, 0.02745098, 0.01960784],\n",
              "         [0.03921569, 0.03529412, 0.02745098],\n",
              "         [0.04313726, 0.03921569, 0.03137255],\n",
              "         ...,\n",
              "         [0.1254902 , 0.10588235, 0.0627451 ],\n",
              "         [0.14117648, 0.11372549, 0.05882353],\n",
              "         [0.16078432, 0.12941177, 0.06666667]],\n",
              "\n",
              "        [[0.03529412, 0.03137255, 0.02352941],\n",
              "         [0.04705882, 0.04313726, 0.03529412],\n",
              "         [0.04705882, 0.04313726, 0.03529412],\n",
              "         ...,\n",
              "         [0.1254902 , 0.09411765, 0.06666667],\n",
              "         [0.14901961, 0.10588235, 0.05098039],\n",
              "         [0.2       , 0.14901961, 0.08627451]],\n",
              "\n",
              "        [[0.03921569, 0.03529412, 0.02745098],\n",
              "         [0.04705882, 0.04313726, 0.03529412],\n",
              "         [0.04705882, 0.04313726, 0.03529412],\n",
              "         ...,\n",
              "         [0.14117648, 0.07843138, 0.07058824],\n",
              "         [0.19215687, 0.1254902 , 0.09803922],\n",
              "         [0.2627451 , 0.19215687, 0.16078432]]],\n",
              "\n",
              "\n",
              "       [[[0.29411766, 0.31764707, 0.1764706 ],\n",
              "         [0.30980393, 0.33333334, 0.19215687],\n",
              "         [0.27450982, 0.29803923, 0.15686275],\n",
              "         ...,\n",
              "         [0.34509805, 0.32941177, 0.21568628],\n",
              "         [0.4       , 0.3764706 , 0.28235295],\n",
              "         [0.42745098, 0.4       , 0.31764707]],\n",
              "\n",
              "        [[0.29803923, 0.32156864, 0.18039216],\n",
              "         [0.28235295, 0.30588236, 0.16470589],\n",
              "         [0.24313726, 0.26666668, 0.1254902 ],\n",
              "         ...,\n",
              "         [0.3254902 , 0.30980393, 0.20784314],\n",
              "         [0.43529412, 0.4117647 , 0.33333334],\n",
              "         [0.34117648, 0.3137255 , 0.2509804 ]],\n",
              "\n",
              "        [[0.30588236, 0.32941177, 0.1882353 ],\n",
              "         [0.2901961 , 0.3137255 , 0.17254902],\n",
              "         [0.2509804 , 0.27450982, 0.13333334],\n",
              "         ...,\n",
              "         [0.38431373, 0.3764706 , 0.28235295],\n",
              "         [0.39607844, 0.3764706 , 0.3254902 ],\n",
              "         [0.16470589, 0.13725491, 0.09803922]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.12941177, 0.11372549, 0.11764706],\n",
              "         [0.12941177, 0.11372549, 0.11764706],\n",
              "         [0.12941177, 0.11372549, 0.11764706],\n",
              "         ...,\n",
              "         [0.16078432, 0.16470589, 0.14117648],\n",
              "         [0.2509804 , 0.25490198, 0.23529412],\n",
              "         [0.32941177, 0.33333334, 0.3137255 ]],\n",
              "\n",
              "        [[0.12941177, 0.11372549, 0.11764706],\n",
              "         [0.12941177, 0.11372549, 0.11764706],\n",
              "         [0.12941177, 0.11372549, 0.11764706],\n",
              "         ...,\n",
              "         [0.3254902 , 0.32941177, 0.30588236],\n",
              "         [0.37254903, 0.3764706 , 0.3529412 ],\n",
              "         [0.3764706 , 0.38039216, 0.36078432]],\n",
              "\n",
              "        [[0.12941177, 0.11372549, 0.11764706],\n",
              "         [0.12941177, 0.11372549, 0.11764706],\n",
              "         [0.12941177, 0.11372549, 0.11764706],\n",
              "         ...,\n",
              "         [0.32941177, 0.33333334, 0.30980393],\n",
              "         [0.3529412 , 0.35686275, 0.3372549 ],\n",
              "         [0.3529412 , 0.35686275, 0.3372549 ]]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_psbd6YV1wg",
        "outputId": "56821cee-a30c-4cbb-b35d-6985454ce59c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_LADLwEV1ti",
        "outputId": "0317df2d-8a6e-4cb0-c44a-4f41cc159ec7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1360, 224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVOKPscYV1qv",
        "outputId": "9a05cb06-da09-469b-acf0-ed73bd294e25"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1360, 17)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# 1st Convolutional Layer\n",
        "model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), strides=(4,4), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Pooling \n",
        "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
        "# Batch Normalisation before passing it to the next layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 2nd Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Pooling\n",
        "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "# 3rd Convolutional Layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 4th Convolutional Layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 5th Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Pooling\n",
        "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Passing it to a dense layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# 1st Dense Layer\n",
        "model.add(Dense(4096, input_shape=(224*224*3,)))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout to prevent overfitting\n",
        "model.add(Dropout(0.4))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 2nd Dense Layer\n",
        "model.add(Dense(4096))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout\n",
        "model.add(Dropout(0.4))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(17))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voxrdWq-V1kJ",
        "outputId": "ed69b037-dfa2-4e4e-e265-0dde5f2d4fb2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/layers/normalization/batch_normalization.py:532: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 54, 54, 96)        34944     \n",
            "                                                                 \n",
            " activation (Activation)     (None, 54, 54, 96)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 26, 26, 96)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 26, 26, 96)       384       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 26, 26, 256)       614656    \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 26, 26, 256)       0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 12, 12, 256)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 12, 12, 256)      1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 10, 10, 384)       885120    \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 10, 10, 384)       0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 10, 10, 384)      1536      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 8, 8, 384)         1327488   \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 8, 8, 384)         0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 8, 8, 384)        1536      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 6, 6, 256)         884992    \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 6, 6, 256)         0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 2, 2, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 2, 2, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4096)              4198400   \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 4096)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 4096)              0         \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 4096)             16384     \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4096)              16781312  \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 4096)              0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 4096)              0         \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 4096)             16384     \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 17)                69649     \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 17)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,834,833\n",
            "Trainable params: 24,815,697\n",
            "Non-trainable params: 19,136\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile \n",
        "opt = tf.optimizers.Adam(learning_rate = 0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "i4l62YpTV1d_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "model.fit(x, y, batch_size=64, epochs=100, verbose=1,validation_split=0.2, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41iIcgVaV1a3",
        "outputId": "6da1875c-2266-4eda-f39a-33eb16d53b7a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 1088 samples, validate on 272 samples\n",
            "Epoch 1/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.4556 - acc: 0.8355 - val_loss: 1.4867 - val_acc: 0.5809\n",
            "Epoch 2/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.4205 - acc: 0.8428 - val_loss: 1.7747 - val_acc: 0.5699\n",
            "Epoch 3/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.3996 - acc: 0.8695 - val_loss: 1.8150 - val_acc: 0.5368\n",
            "Epoch 4/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.4406 - acc: 0.8493 - val_loss: 2.6064 - val_acc: 0.4118\n",
            "Epoch 5/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.4861 - acc: 0.8382 - val_loss: 2.0791 - val_acc: 0.5037\n",
            "Epoch 6/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.3769 - acc: 0.8732 - val_loss: 1.6984 - val_acc: 0.5735\n",
            "Epoch 7/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.4960 - acc: 0.8336 - val_loss: 1.6173 - val_acc: 0.6324\n",
            "Epoch 8/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.3733 - acc: 0.8695 - val_loss: 1.8530 - val_acc: 0.5919\n",
            "Epoch 9/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.3880 - acc: 0.8824 - val_loss: 1.6822 - val_acc: 0.5956\n",
            "Epoch 10/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.3243 - acc: 0.8915 - val_loss: 1.9448 - val_acc: 0.5662\n",
            "Epoch 11/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.3083 - acc: 0.8934 - val_loss: 1.8213 - val_acc: 0.6176\n",
            "Epoch 12/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.2602 - acc: 0.9191 - val_loss: 2.0883 - val_acc: 0.5956\n",
            "Epoch 13/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.3170 - acc: 0.9044 - val_loss: 6.5781 - val_acc: 0.3309\n",
            "Epoch 14/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.4251 - acc: 0.8658 - val_loss: 2.0784 - val_acc: 0.6029\n",
            "Epoch 15/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.3799 - acc: 0.8833 - val_loss: 2.8922 - val_acc: 0.5441\n",
            "Epoch 16/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.3732 - acc: 0.8750 - val_loss: 3.6697 - val_acc: 0.6066\n",
            "Epoch 17/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.3268 - acc: 0.8961 - val_loss: 3.5786 - val_acc: 0.5882\n",
            "Epoch 18/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.3141 - acc: 0.8897 - val_loss: 2.2203 - val_acc: 0.6324\n",
            "Epoch 19/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.2873 - acc: 0.9090 - val_loss: 1.8341 - val_acc: 0.6360\n",
            "Epoch 20/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.2237 - acc: 0.9246 - val_loss: 2.2814 - val_acc: 0.6140\n",
            "Epoch 21/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.2744 - acc: 0.9118 - val_loss: 2.7664 - val_acc: 0.5919\n",
            "Epoch 22/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.2408 - acc: 0.9210 - val_loss: 1.9362 - val_acc: 0.6434\n",
            "Epoch 23/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.2089 - acc: 0.9320 - val_loss: 1.8015 - val_acc: 0.6434\n",
            "Epoch 24/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1683 - acc: 0.9403 - val_loss: 1.9251 - val_acc: 0.6691\n",
            "Epoch 25/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1370 - acc: 0.9559 - val_loss: 2.0352 - val_acc: 0.6691\n",
            "Epoch 26/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1598 - acc: 0.9531 - val_loss: 2.2611 - val_acc: 0.6434\n",
            "Epoch 27/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1431 - acc: 0.9540 - val_loss: 2.1153 - val_acc: 0.6140\n",
            "Epoch 28/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1569 - acc: 0.9494 - val_loss: 2.2259 - val_acc: 0.6250\n",
            "Epoch 29/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1632 - acc: 0.9449 - val_loss: 8.2631 - val_acc: 0.6324\n",
            "Epoch 30/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1509 - acc: 0.9458 - val_loss: 10.2858 - val_acc: 0.5919\n",
            "Epoch 31/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1162 - acc: 0.9605 - val_loss: 9.2685 - val_acc: 0.6103\n",
            "Epoch 32/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1111 - acc: 0.9632 - val_loss: 7.1702 - val_acc: 0.5404\n",
            "Epoch 33/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1924 - acc: 0.9347 - val_loss: 7.0791 - val_acc: 0.5846\n",
            "Epoch 34/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1845 - acc: 0.9403 - val_loss: 7.9271 - val_acc: 0.5294\n",
            "Epoch 35/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1993 - acc: 0.9347 - val_loss: 7.7242 - val_acc: 0.6213\n",
            "Epoch 36/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1981 - acc: 0.9375 - val_loss: 8.2265 - val_acc: 0.6544\n",
            "Epoch 37/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1052 - acc: 0.9688 - val_loss: 8.8502 - val_acc: 0.5699\n",
            "Epoch 38/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1304 - acc: 0.9605 - val_loss: 8.4913 - val_acc: 0.6544\n",
            "Epoch 39/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1119 - acc: 0.9623 - val_loss: 7.3355 - val_acc: 0.5846\n",
            "Epoch 40/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0959 - acc: 0.9715 - val_loss: 5.8585 - val_acc: 0.6324\n",
            "Epoch 41/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1038 - acc: 0.9706 - val_loss: 7.6854 - val_acc: 0.6507\n",
            "Epoch 42/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1282 - acc: 0.9623 - val_loss: 14.6272 - val_acc: 0.6103\n",
            "Epoch 43/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.2066 - acc: 0.9430 - val_loss: 25.2648 - val_acc: 0.4890\n",
            "Epoch 44/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1270 - acc: 0.9577 - val_loss: 23.2862 - val_acc: 0.6618\n",
            "Epoch 45/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1254 - acc: 0.9623 - val_loss: 16.0915 - val_acc: 0.6801\n",
            "Epoch 46/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0863 - acc: 0.9733 - val_loss: 21.9783 - val_acc: 0.6471\n",
            "Epoch 47/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0868 - acc: 0.9669 - val_loss: 15.3950 - val_acc: 0.6949\n",
            "Epoch 48/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0705 - acc: 0.9770 - val_loss: 25.2407 - val_acc: 0.6838\n",
            "Epoch 49/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0795 - acc: 0.9798 - val_loss: 27.5593 - val_acc: 0.6287\n",
            "Epoch 50/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0414 - acc: 0.9835 - val_loss: 13.4308 - val_acc: 0.6728\n",
            "Epoch 51/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0671 - acc: 0.9816 - val_loss: 19.6063 - val_acc: 0.7022\n",
            "Epoch 52/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0689 - acc: 0.9807 - val_loss: 26.1888 - val_acc: 0.7132\n",
            "Epoch 53/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1016 - acc: 0.9697 - val_loss: 23.3027 - val_acc: 0.5993\n",
            "Epoch 54/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0839 - acc: 0.9669 - val_loss: 30.7231 - val_acc: 0.5956\n",
            "Epoch 55/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1949 - acc: 0.9485 - val_loss: 30.2535 - val_acc: 0.6140\n",
            "Epoch 56/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.2031 - acc: 0.9412 - val_loss: 38.9298 - val_acc: 0.5257\n",
            "Epoch 57/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1351 - acc: 0.9559 - val_loss: 46.0859 - val_acc: 0.6691\n",
            "Epoch 58/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1214 - acc: 0.9623 - val_loss: 64.3123 - val_acc: 0.6397\n",
            "Epoch 59/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0721 - acc: 0.9761 - val_loss: 78.6279 - val_acc: 0.5919\n",
            "Epoch 60/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0791 - acc: 0.9770 - val_loss: 90.0327 - val_acc: 0.6728\n",
            "Epoch 61/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0611 - acc: 0.9807 - val_loss: 99.5788 - val_acc: 0.6765\n",
            "Epoch 62/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0479 - acc: 0.9816 - val_loss: 100.0628 - val_acc: 0.7206\n",
            "Epoch 63/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0510 - acc: 0.9835 - val_loss: 95.8512 - val_acc: 0.6691\n",
            "Epoch 64/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0849 - acc: 0.9697 - val_loss: 117.3804 - val_acc: 0.6875\n",
            "Epoch 65/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1344 - acc: 0.9614 - val_loss: 143.7248 - val_acc: 0.6213\n",
            "Epoch 66/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0971 - acc: 0.9660 - val_loss: 123.1187 - val_acc: 0.5956\n",
            "Epoch 67/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0804 - acc: 0.9770 - val_loss: 214.2482 - val_acc: 0.6507\n",
            "Epoch 68/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0329 - acc: 0.9862 - val_loss: 292.0014 - val_acc: 0.5257\n",
            "Epoch 69/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0593 - acc: 0.9816 - val_loss: 304.5816 - val_acc: 0.6434\n",
            "Epoch 70/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0627 - acc: 0.9779 - val_loss: 548.7562 - val_acc: 0.6324\n",
            "Epoch 71/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0515 - acc: 0.9807 - val_loss: 576.8905 - val_acc: 0.7022\n",
            "Epoch 72/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0492 - acc: 0.9835 - val_loss: 806.8068 - val_acc: 0.6397\n",
            "Epoch 73/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0304 - acc: 0.9926 - val_loss: 1239.6347 - val_acc: 0.6397\n",
            "Epoch 74/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0585 - acc: 0.9853 - val_loss: 1176.1315 - val_acc: 0.6507\n",
            "Epoch 75/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0763 - acc: 0.9752 - val_loss: 1473.0367 - val_acc: 0.6985\n",
            "Epoch 76/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0815 - acc: 0.9733 - val_loss: 1077.2207 - val_acc: 0.4191\n",
            "Epoch 77/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0706 - acc: 0.9724 - val_loss: 1292.8381 - val_acc: 0.6250\n",
            "Epoch 78/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1185 - acc: 0.9715 - val_loss: 922.3586 - val_acc: 0.6213\n",
            "Epoch 79/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1062 - acc: 0.9651 - val_loss: 1285.3324 - val_acc: 0.5294\n",
            "Epoch 80/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1604 - acc: 0.9540 - val_loss: 1491.9292 - val_acc: 0.6029\n",
            "Epoch 81/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1135 - acc: 0.9623 - val_loss: 1194.7537 - val_acc: 0.6985\n",
            "Epoch 82/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1076 - acc: 0.9688 - val_loss: 1348.8555 - val_acc: 0.6728\n",
            "Epoch 83/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1121 - acc: 0.9632 - val_loss: 1932.7403 - val_acc: 0.6287\n",
            "Epoch 84/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1088 - acc: 0.9660 - val_loss: 3938.5824 - val_acc: 0.6140\n",
            "Epoch 85/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1180 - acc: 0.9642 - val_loss: 5448.8424 - val_acc: 0.5846\n",
            "Epoch 86/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0719 - acc: 0.9733 - val_loss: 6634.8996 - val_acc: 0.6213\n",
            "Epoch 87/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0546 - acc: 0.9807 - val_loss: 6156.8794 - val_acc: 0.6250\n",
            "Epoch 88/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0686 - acc: 0.9835 - val_loss: 6646.3568 - val_acc: 0.6324\n",
            "Epoch 89/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0712 - acc: 0.9825 - val_loss: 7676.7717 - val_acc: 0.6581\n",
            "Epoch 90/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0588 - acc: 0.9871 - val_loss: 9944.1495 - val_acc: 0.6029\n",
            "Epoch 91/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0783 - acc: 0.9770 - val_loss: 11603.2661 - val_acc: 0.5147\n",
            "Epoch 92/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0930 - acc: 0.9706 - val_loss: 12059.7673 - val_acc: 0.6029\n",
            "Epoch 93/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0936 - acc: 0.9715 - val_loss: 12258.8287 - val_acc: 0.6471\n",
            "Epoch 94/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0515 - acc: 0.9798 - val_loss: 11689.0651 - val_acc: 0.6434\n",
            "Epoch 95/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0486 - acc: 0.9853 - val_loss: 14605.7455 - val_acc: 0.6507\n",
            "Epoch 96/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.0589 - acc: 0.9835 - val_loss: 14725.6902 - val_acc: 0.6801\n",
            "Epoch 97/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1145 - acc: 0.9715 - val_loss: 17515.0970 - val_acc: 0.6838\n",
            "Epoch 98/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1013 - acc: 0.9761 - val_loss: 15625.5324 - val_acc: 0.6029\n",
            "Epoch 99/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1068 - acc: 0.9752 - val_loss: 15745.3695 - val_acc: 0.5993\n",
            "Epoch 100/100\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 0.1516 - acc: 0.9577 - val_loss: 11505.9543 - val_acc: 0.6397\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff8f00e2a50>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ftWJpCQ-V1X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZOQEwWPAV1Uh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}